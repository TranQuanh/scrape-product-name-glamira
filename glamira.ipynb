{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import random\n",
    "import concurrent.futures\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "total_image = 0  # Global variable to count total images downloaded\n",
    "\n",
    "products_dict = {}\n",
    "product_errors = {}\n",
    "def update_json_all_file(file_path, new_data):\n",
    "    # Kiểm tra xem tệp có tồn tại không\n",
    "    if os.path.exists(file_path):\n",
    "        # Đọc dữ liệu hiện có từ tệp\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = {}\n",
    "    else:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Kiểm tra nếu dữ liệu hiện có không phải là dict, biến nó thành dict\n",
    "    if not isinstance(existing_data, dict):\n",
    "        existing_data = {}\n",
    "    for key,value in new_data.items():\n",
    "        if key  in existing_data:\n",
    "            existing_data[key].update(value)\n",
    "        else:\n",
    "            existing_data.update(new_data)    \n",
    "    \n",
    "\n",
    "    # Ghi dữ liệu đã cập nhật trở lại tệp\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def update_json_error(file_path, new_data):\n",
    "    # Kiểm tra xem tệp có tồn tại không\n",
    "    if os.path.exists(file_path):\n",
    "        # Đọc dữ liệu hiện có từ tệp\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = {}\n",
    "    else:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Kiểm tra nếu dữ liệu hiện có không phải là dict, biến nó thành dict\n",
    "    if not isinstance(existing_data, dict):\n",
    "        existing_data = {}\n",
    "\n",
    "    existing_data.update(new_data)    \n",
    "    \n",
    "\n",
    "    # Ghi dữ liệu đã cập nhật trở lại tệp\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"product_details.json\"\n",
    "with open(file_path,'r',encoding='utf-8') as file:\n",
    "    try:\n",
    "        products_dict = json.load(file)\n",
    "    except:\n",
    "        products_dict = {}\n",
    "print(len(products_dict))\n",
    "\n",
    "file_path = \"product_category_error.jsonl\"\n",
    "with open(file_path,'r',encoding='utf-8') as file:\n",
    "    try:\n",
    "        products_error = json.load(file)\n",
    "    except:\n",
    "        products_error = {}\n",
    "print(len(products_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\V'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\V'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25652\\3713987332.py:14: SyntaxWarning: invalid escape sequence '\\V'\n",
      "  folder = 'D:\\VScode\\DE\\crawl_data\\Beautifulsoup\\Glamira\\project_glamira\\photo'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/brown-diamond/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/citrine/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/diamond/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/white-sapphire/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/swarovski-crystal/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/emerald/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/swarovski-green/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/fire-opal/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/garnet/\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/green-amethyst/\n",
      "Total images downloaded: 1\n",
      "Total images downloaded: 2\n",
      "Total images downloaded: 3\n",
      "Total images downloaded: 4\n",
      "Total images downloaded: 5\n",
      "Total images downloaded: 6\n",
      "Total images downloaded: 7\n",
      "Total images downloaded: 8\n",
      "Total images downloaded: 9\n",
      "Total images downloaded: 10\n",
      "Total images downloaded: 11\n",
      "Total images downloaded: 12\n",
      "Total images downloaded: 13\n",
      "Total images downloaded: 14\n",
      "Total images downloaded: 15\n",
      "Total images downloaded: 16\n",
      "Total images downloaded: 17\n",
      "Total images downloaded: 18\n",
      "Total images downloaded: 19\n",
      "Total images downloaded: 20\n",
      "Total images downloaded: 21\n",
      "Total images downloaded: 22\n",
      "Total images downloaded: 23\n",
      "Total images downloaded: 24\n",
      "Total images downloaded: 25\n",
      "Sleeping for 3-5 seconds...\n",
      "Total images downloaded: 26\n",
      "Total images downloaded: 27\n",
      "Total images downloaded: 28\n",
      "Total images downloaded: 29\n",
      "Total images downloaded: 30\n",
      "Total images downloaded: 31\n",
      "Total images downloaded: 32\n",
      "Total images downloaded: 33\n",
      "Total images downloaded: 34\n",
      "Total images downloaded: 35\n",
      "Total images downloaded: 36\n",
      "Total images downloaded: 37\n",
      "Total images downloaded: 38\n",
      "Total images downloaded: 39\n",
      "Total images downloaded: 40\n",
      "Total images downloaded: 41\n",
      "Total images downloaded: 42\n",
      "Total images downloaded: 43\n",
      "Total images downloaded: 44\n",
      "Total images downloaded: 45\n",
      "Total images downloaded: 46\n",
      "Total images downloaded: 47\n",
      "Total images downloaded: 48\n",
      "Total images downloaded: 49\n",
      "Total images downloaded: 50\n",
      "Sleeping for 3-5 seconds...\n",
      "Total images downloaded: 51\n",
      "Total images downloaded: 52\n",
      "Total images downloaded: 53\n",
      "Total images downloaded: 54\n",
      "Total images downloaded: 55\n",
      "Total images downloaded: 56\n",
      "Total images downloaded: 57\n",
      "Total images downloaded: 58\n",
      "Total images downloaded: 59\n",
      "Total images downloaded: 60\n",
      "Total images downloaded: 61\n",
      "Total images downloaded: 62\n",
      "Total images downloaded: 63\n",
      "Total images downloaded: 64\n",
      "Total images downloaded: 65\n",
      "Total images downloaded: 66\n",
      "Total images downloaded: 67\n",
      "Total images downloaded: 68\n",
      "Total images downloaded: 69\n",
      "Total images downloaded: 70\n",
      "Total images downloaded: 71\n",
      "Total images downloaded: 72\n",
      "Total images downloaded: 73\n",
      "Total images downloaded: 74\n",
      "Total images downloaded: 75\n",
      "Sleeping for 3-5 seconds...\n",
      "Total images downloaded: 76\n",
      "Total images downloaded: 77\n",
      "Total images downloaded: 78\n",
      "Total images downloaded: 79\n",
      "Total images downloaded: 80\n",
      "Total images downloaded: 81\n",
      "Total images downloaded: 82\n",
      "Total images downloaded: 83\n",
      "Total images downloaded: 84\n",
      "Total images downloaded: 85\n",
      "Total images downloaded: 86\n",
      "Total images downloaded: 87\n",
      "Total images downloaded: 88\n",
      "Total images downloaded: 89\n",
      "Total images downloaded: 90\n",
      "Total images downloaded: 91\n",
      "Total images downloaded: 92\n",
      "Total images downloaded: 93\n",
      "Total images downloaded: 94\n",
      "Total images downloaded: 95\n",
      "Total images downloaded: 96\n",
      "Total images downloaded: 97\n",
      "Total images downloaded: 98\n",
      "Total images downloaded: 99\n",
      "Total images downloaded: 100\n",
      "Sleeping for 3-5 seconds...\n",
      "Total images downloaded: 101\n",
      "Total images downloaded: 102\n",
      "Total images downloaded: 103\n",
      "Total images downloaded: 104\n",
      "Total images downloaded: 105\n",
      "Total images downloaded: 106\n",
      "Total images downloaded: 107\n",
      "Total images downloaded: 108\n",
      "Total images downloaded: 109\n",
      "Total images downloaded: 110\n",
      "Total images downloaded: 111\n",
      "Total images downloaded: 112\n",
      "Total images downloaded: 113\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/green-amethyst/ details saved to product_details.json\n",
      "https://www.glamira.com/hand-of-hamsa-collection-earrings/green-sapphire/\n"
     ]
    }
   ],
   "source": [
    "def extract_key(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    path_components = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "    return path_components[0]\n",
    "\n",
    "def sanitize_file_name(text):\n",
    "    text = text.replace(\" \", \"-\")\n",
    "    text = re.sub(r'[^A-Za-z0-9_-]', '', text)\n",
    "    return text\n",
    "\n",
    "def download_image(image_link, product_id, hash_product_name, view_number,param):\n",
    "    global total_image  # Declare total_image as global to modify it\n",
    "    \n",
    "    folder = 'D:\\VScode\\DE\\crawl_data\\Beautifulsoup\\Glamira\\project_glamira\\photo'\n",
    "    file_name = f\"image_{product_id}_{hash_product_name}_{param}_{view_number}\"\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    \n",
    "    resp = requests.get(image_link)\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        with open(f\"{file_path}.jpg\", 'wb') as file:\n",
    "            file.write(resp.content)\n",
    "        \n",
    "        total_image += 1  # Increment total_image\n",
    "        print(f\"Total images downloaded: {total_image}\")\n",
    "        \n",
    "        #Every 25 links image, sleep 3-5 seconds\n",
    "        if total_image % 25 == 0:\n",
    "            print(\"Sleeping for 3-5 seconds...\")\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve the image {image_link}\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "def generate_image(image_first_link, product_id, hash_product_name,param):\n",
    "    parts = image_first_link.split('/view/')\n",
    "    base_url_part1 = parts[0] + '/view/'\n",
    "    base_url_part2 = parts[1].split('/', 1)[1]\n",
    "    \n",
    "    image_download_paths = []\n",
    "    \n",
    "    for view_number in range(1, 5):\n",
    "        image_view_link = f\"{base_url_part1}{view_number}/{base_url_part2}\"\n",
    "        image_download = download_image(image_view_link, product_id, hash_product_name, view_number,param)\n",
    "        image_download_paths.append(image_download)\n",
    "\n",
    "    return image_download_paths\n",
    "\n",
    "def take_product_info(soup, products_dict):\n",
    "    product_items = soup.find_all('li', class_=['item', 'product', 'product-item'])\n",
    "\n",
    "    for item in product_items:\n",
    "        try:\n",
    "            category_label = item.find('p', class_='enable-popover popover_stone_info')['data-ajax-data']\n",
    "            category_json = json.loads(category_label)\n",
    "            category_id = category_json['data_filter']['id']\n",
    "        except:\n",
    "            category_id = \"No Found Category ID\"\n",
    "\n",
    "        try:\n",
    "            product_id = item.find('div', class_='price-box price-final_price')['data-product-id']\n",
    "        except:\n",
    "            product_id = \"No Found Product ID\"\n",
    "\n",
    "        try:\n",
    "            product_name = item.find('h2', class_=\"product-name\").get_text()\n",
    "            hash_product_name = sanitize_file_name(product_name)\n",
    "        except:\n",
    "            product_name = \"No Found Product Name\"\n",
    "\n",
    "        try:\n",
    "            image_link = item.find('img')['srcset'].split(',')[0].split()[0]\n",
    "            image_path = generate_image(image_link, product_id, hash_product_name)\n",
    "        except:\n",
    "            image_path = \"No found images\"\n",
    "            \n",
    "        try:\n",
    "            product_link = item.find('a', class_='product-link')['href']\n",
    "            param = item.find('a', class_='product-link')['data-param']\n",
    "            param_dict = json.loads(param)\n",
    "            json_str = urllib.parse.urlencode(param_dict)\n",
    "            product_full_link = product_link + '?' + json_str\n",
    "        except:\n",
    "            product_link = \"No Found Product Link\"\n",
    "\n",
    "        try:\n",
    "            short_description = item.find('span', class_='short-description').get_text(strip=True)\n",
    "        except:\n",
    "            short_description = \"No Found Short Description\"\n",
    "\n",
    "        try:\n",
    "            carat_info = item.find('span', class_='carat').get_text(strip=True)\n",
    "        except:\n",
    "            carat_info = \"No Found Carat Info\"\n",
    "\n",
    "        try:\n",
    "            price = item.find('span', class_='price').get_text(strip=True)\n",
    "        except:\n",
    "            price = \"No Found Price \"\n",
    "\n",
    "        try:\n",
    "            product_info = json.loads(item.find('a', class_='product-link')['data-param'])\n",
    "        except:\n",
    "            product_info = \"No Found Details\"\n",
    "\n",
    "        try:\n",
    "            image_link = item.find('img')['srcset'].split(',')[0].split()[0]\n",
    "            image_path = generate_image(image_link, product_id, hash_product_name,json_str)\n",
    "        except:\n",
    "            image_path = \"No found images\"\n",
    "\n",
    "        product_details = {\n",
    "            \"Product Name\": product_name,\n",
    "            \"Product ID\": product_id,\n",
    "            'Product Full Link': product_full_link,\n",
    "            \"Category ID\": category_id,\n",
    "            'Price': price,\n",
    "            'Carat': carat_info,\n",
    "            \"Detail\": product_info,\n",
    "            \"Descript\": short_description,\n",
    "            'Product Image Path': image_path\n",
    "        }\n",
    "\n",
    "        if product_name not in products_dict:\n",
    "            products_dict[product_name] = {short_description: product_details}\n",
    "        else:\n",
    "            products_dict[product_name][short_description] = product_details\n",
    "\n",
    "def get_api_links(link_category):\n",
    "    \n",
    "    api_link = link_category + \"?isAjax=true&sections=list_product\"\n",
    "    \n",
    "    print(link_category)\n",
    "\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        retry_strategy = requests.packages.urllib3.util.retry.Retry(\n",
    "            total=3,  # Number of retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # HTTP statuses to retry on\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],  # Methods to retry\n",
    "            backoff_factor=random.uniform(0, 10)  # Retry delay in seconds\n",
    "        )\n",
    "        adapter = requests.adapters.HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.mount(\"http://\", adapter)\n",
    "\n",
    "        api_response = session.get(api_link, timeout=10)\n",
    "        api_response.raise_for_status()  # Raise HTTPError for bad responses \n",
    "\n",
    "        if api_response.status_code == 200:\n",
    "            api_content = api_response.content\n",
    "            data = json.loads(api_content)\n",
    "            html_content = data[\"list_product\"]\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            take_product_info(soup, products_dict)\n",
    "            last_page = int(soup.find('li', class_=['item', 'product', 'product-item'])['data-lastpage'])\n",
    "            \n",
    "            if last_page == 1:\n",
    "                pass\n",
    "            \n",
    "            for i in range(2, last_page + 1):\n",
    "                api_link_2 = f\"{link_category}?p={i}&isAjax=true&sections=list_product\"\n",
    "                api_response = requests.get(api_link_2)\n",
    "                api_content = api_response.content\n",
    "                data = json.loads(api_content)\n",
    "                html_content = data[\"list_product\"]\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                take_product_info(soup, products_dict)\n",
    "\n",
    "            # Save products_dict to JSON file\n",
    "            output_file = 'product_details.json'\n",
    "            update_json_all_file(output_file,products_dict)\n",
    "\n",
    "            print(f\"{link_category} details saved to {output_file}\")\n",
    "            return products_dict\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {api_link}. Status code: {api_response.status_code}\")\n",
    "            output_file = 'product_category_error.jsonl'\n",
    "            update_json_error(output_file,'link error')\n",
    "            return {}\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching initial data for {link_category}: {e}\")\n",
    "        output_file = 'product_category_error.jsonl'\n",
    "        update_json_error(output_file,'link error')\n",
    "        return {}\n",
    "\n",
    "\n",
    "# List of XML files containing API links\n",
    "xml_files = [\n",
    "    'https://www.glamira.com/media/sitemap/glus/category_filter_provider-41-2.xml'\n",
    "]\n",
    "\n",
    "links = []\n",
    "\n",
    "# Extract all https links from XML files\n",
    "for xml_file in xml_files:\n",
    "    response = requests.get(xml_file)\n",
    "    xml_content = response.content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    for element in root.iter():\n",
    "        if element.text is not None and element.text.startswith(\"https:\") and \"carat\" not in element.text:\n",
    "            links.append(element.text)\n",
    "\n",
    "\n",
    "\n",
    "# Process API links with thread method\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(get_api_links, link) for link in links[800:850]]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(\"Image Download Counts per link_category:\")\n",
    "            print(f\"Total images downloaded: {total_image}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
